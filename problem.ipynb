{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtKfdn1f4EpG"
      },
      "source": [
        "# Задание по курсу \"Практическое введение в анализ изображений\"\n",
        "\n",
        "Задание посвящено разработке метода автоматической сегментации минералов на микроскопических изображениях геологических аншлифов. Задание не имеет никаких ограничений по используемым подходам и методам. Тем не менее большинство вещей, расказанных в курсе \"Практическое введение в анализ изображений\", так или иначе связаны с тематикой задания и могут быть полезны.\n",
        "\n",
        "В данной реализации используется пакет [`petroscope`](https://github.com/xubiker/petroscope), содержащий набор методов, упрощающих работу с геологическими изображениями. \n",
        "\n",
        "В качестве данных для обучения и тестирование в данном задании используется набор изображений [LumenStone](https://imaging.cs.msu.ru/en/research/geology/lumenstone) (подмножество S1v1.5).\n",
        "\n",
        "\n",
        "\n",
        "Используется реализация классических суперпиксельных алгоритмов из пакета `scikit-image`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-WA8WOi4EpH"
      },
      "source": [
        "Для начала установим пакет `petroscope`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EKyz3in_4EpI",
        "outputId": "a1dc5b94-d5b2-4472-df35-7a9e00dd24db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting petroscope\n",
            "  Using cached petroscope-0.0.11-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting pyyaml (from petroscope)\n",
            "  Using cached PyYAML-6.0.2-cp313-cp313-macosx_10_13_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting numpy<2.0.0,>=1.16 (from petroscope)\n",
            "  Using cached numpy-1.26.4-cp313-cp313-macosx_11_0_x86_64.whl\n",
            "Collecting pillow (from petroscope)\n",
            "  Using cached pillow-11.1.0-cp313-cp313-macosx_10_13_x86_64.whl.metadata (9.1 kB)\n",
            "Collecting matplotlib (from petroscope)\n",
            "  Using cached matplotlib-3.10.1-cp313-cp313-macosx_10_13_x86_64.whl.metadata (11 kB)\n",
            "Collecting tqdm (from petroscope)\n",
            "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting scipy (from petroscope)\n",
            "  Using cached scipy-1.15.2-cp313-cp313-macosx_10_13_x86_64.whl.metadata (61 kB)\n",
            "Collecting loguru (from petroscope)\n",
            "  Using cached loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting prettytable (from petroscope)\n",
            "  Using cached prettytable-3.16.0-py3-none-any.whl.metadata (33 kB)\n",
            "Collecting requests (from petroscope)\n",
            "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib->petroscope)\n",
            "  Using cached contourpy-1.3.1-cp313-cp313-macosx_10_13_x86_64.whl.metadata (5.4 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib->petroscope)\n",
            "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib->petroscope)\n",
            "  Downloading fonttools-4.57.0-cp313-cp313-macosx_10_13_x86_64.whl.metadata (102 kB)\n",
            "Collecting kiwisolver>=1.3.1 (from matplotlib->petroscope)\n",
            "  Using cached kiwisolver-1.4.8-cp313-cp313-macosx_10_13_x86_64.whl.metadata (6.2 kB)\n",
            "Collecting packaging>=20.0 (from matplotlib->petroscope)\n",
            "  Using cached packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting pyparsing>=2.3.1 (from matplotlib->petroscope)\n",
            "  Using cached pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting python-dateutil>=2.7 (from matplotlib->petroscope)\n",
            "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting wcwidth (from prettytable->petroscope)\n",
            "  Using cached wcwidth-0.2.13-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests->petroscope)\n",
            "  Using cached charset_normalizer-3.4.1-cp313-cp313-macosx_10_13_universal2.whl.metadata (35 kB)\n",
            "Collecting idna<4,>=2.5 (from requests->petroscope)\n",
            "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests->petroscope)\n",
            "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests->petroscope)\n",
            "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting six>=1.5 (from python-dateutil>=2.7->matplotlib->petroscope)\n",
            "  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Using cached petroscope-0.0.11-py3-none-any.whl (68 kB)\n",
            "Using cached loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "Using cached matplotlib-3.10.1-cp313-cp313-macosx_10_13_x86_64.whl (8.2 MB)\n",
            "Using cached pillow-11.1.0-cp313-cp313-macosx_10_13_x86_64.whl (3.2 MB)\n",
            "Using cached prettytable-3.16.0-py3-none-any.whl (33 kB)\n",
            "Using cached PyYAML-6.0.2-cp313-cp313-macosx_10_13_x86_64.whl (181 kB)\n",
            "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "Using cached scipy-1.15.2-cp313-cp313-macosx_10_13_x86_64.whl (38.7 MB)\n",
            "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
            "Using cached charset_normalizer-3.4.1-cp313-cp313-macosx_10_13_universal2.whl (195 kB)\n",
            "Using cached contourpy-1.3.1-cp313-cp313-macosx_10_13_x86_64.whl (271 kB)\n",
            "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading fonttools-4.57.0-cp313-cp313-macosx_10_13_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached idna-3.10-py3-none-any.whl (70 kB)\n",
            "Using cached kiwisolver-1.4.8-cp313-cp313-macosx_10_13_x86_64.whl (66 kB)\n",
            "Using cached packaging-24.2-py3-none-any.whl (65 kB)\n",
            "Using cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
            "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
            "Using cached wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)\n",
            "Using cached six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: wcwidth, urllib3, tqdm, six, pyyaml, pyparsing, prettytable, pillow, packaging, numpy, loguru, kiwisolver, idna, fonttools, cycler, charset-normalizer, certifi, scipy, requests, python-dateutil, contourpy, matplotlib, petroscope\n",
            "  Attempting uninstall: wcwidth\n",
            "    Found existing installation: wcwidth 0.2.13\n",
            "    Uninstalling wcwidth-0.2.13:\n",
            "      Successfully uninstalled wcwidth-0.2.13\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.3.0\n",
            "    Uninstalling urllib3-2.3.0:\n",
            "      Successfully uninstalled urllib3-2.3.0\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.67.1\n",
            "    Uninstalling tqdm-4.67.1:\n",
            "      Successfully uninstalled tqdm-4.67.1\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.17.0\n",
            "    Uninstalling six-1.17.0:\n",
            "      Successfully uninstalled six-1.17.0\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0.2\n",
            "    Uninstalling PyYAML-6.0.2:\n",
            "      Successfully uninstalled PyYAML-6.0.2\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.2.3\n",
            "    Uninstalling pyparsing-3.2.3:\n",
            "      Successfully uninstalled pyparsing-3.2.3\n",
            "  Attempting uninstall: prettytable\n",
            "    Found existing installation: prettytable 3.16.0\n",
            "    Uninstalling prettytable-3.16.0:\n",
            "      Successfully uninstalled prettytable-3.16.0\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.1.0\n",
            "    Uninstalling pillow-11.1.0:\n",
            "      Successfully uninstalled pillow-11.1.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.2\n",
            "    Uninstalling packaging-24.2:\n",
            "      Successfully uninstalled packaging-24.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: loguru\n",
            "    Found existing installation: loguru 0.7.3\n",
            "    Uninstalling loguru-0.7.3:\n",
            "      Successfully uninstalled loguru-0.7.3\n",
            "  Attempting uninstall: kiwisolver\n",
            "    Found existing installation: kiwisolver 1.4.8\n",
            "    Uninstalling kiwisolver-1.4.8:\n",
            "      Successfully uninstalled kiwisolver-1.4.8\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: fonttools\n",
            "    Found existing installation: fonttools 4.56.0\n",
            "    Uninstalling fonttools-4.56.0:\n",
            "      Successfully uninstalled fonttools-4.56.0\n",
            "  Attempting uninstall: cycler\n",
            "    Found existing installation: cycler 0.12.1\n",
            "    Uninstalling cycler-0.12.1:\n",
            "      Successfully uninstalled cycler-0.12.1\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 3.4.1\n",
            "    Uninstalling charset-normalizer-3.4.1:\n",
            "      Successfully uninstalled charset-normalizer-3.4.1\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2025.1.31\n",
            "    Uninstalling certifi-2025.1.31:\n",
            "      Successfully uninstalled certifi-2025.1.31\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.2\n",
            "    Uninstalling scipy-1.15.2:\n",
            "      Successfully uninstalled scipy-1.15.2\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.3\n",
            "    Uninstalling requests-2.32.3:\n",
            "      Successfully uninstalled requests-2.32.3\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.9.0.post0\n",
            "    Uninstalling python-dateutil-2.9.0.post0:\n",
            "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
            "  Attempting uninstall: contourpy\n",
            "    Found existing installation: contourpy 1.3.1\n",
            "    Uninstalling contourpy-1.3.1:\n",
            "      Successfully uninstalled contourpy-1.3.1\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.10.1\n",
            "    Uninstalling matplotlib-3.10.1:\n",
            "      Successfully uninstalled matplotlib-3.10.1\n",
            "  Attempting uninstall: petroscope\n",
            "    Found existing installation: petroscope 0.0.11\n",
            "    Uninstalling petroscope-0.0.11:\n",
            "      Successfully uninstalled petroscope-0.0.11\n",
            "Successfully installed certifi-2025.1.31 charset-normalizer-3.4.1 contourpy-1.3.1 cycler-0.12.1 fonttools-4.57.0 idna-3.10 kiwisolver-1.4.8 loguru-0.7.3 matplotlib-3.10.1 numpy-1.26.4 packaging-24.2 petroscope-0.0.11 pillow-11.1.0 prettytable-3.16.0 pyparsing-3.2.3 python-dateutil-2.9.0.post0 pyyaml-6.0.2 requests-2.32.3 scipy-1.15.2 six-1.17.0 tqdm-4.67.1 urllib3-2.3.0 wcwidth-0.2.13\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install petroscope --force-reinstall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRtCJn9J4EpI"
      },
      "source": [
        "код для отображения изображений в jupiter notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {
        "id": "Rpg7CQPo4EpI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show(image, title: str = None, cmap: str = None):\n",
        "    plt.imshow(image, cmap=cmap)\n",
        "    if title:\n",
        "        plt.title(title)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GitCLCbf5m6W"
      },
      "source": [
        "Пути до датасета LumenStone S1v1.5 и сбалансированных данных. Запись локально."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 294,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jiGzg34o5iY1",
        "outputId": "5ab22f5c-7836-4327-f2f5-1abdf30b4872"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "ds_path = Path('./LumenStone/S1_v1.5/')\n",
        "b_path = Path(\"./balanced_data/\")\n",
        "sb_path = Path(\"./small_balanced_data/\")\n",
        "vsb_path = Path(\"./very_small_balanced_data/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kw4dksf4EpJ"
      },
      "source": [
        "информация о классах (номер класса, аббревиатура, название, цвет маски):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikoTCpCu4EpJ",
        "outputId": "fcbe9d04-413c-4c24-8e04-e5e009c57448"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, bg (background), color: #000000]\n",
            "[1, ccp/kub (chalcopyrite/cubanite), color: #ffa500]\n",
            "[2, gl (galena), color: #9acd32]\n",
            "[4, brt (bornite), color: #00bfff]\n",
            "[6, py/mrc (pyrite/marcasite), color: #2f4f4f]\n",
            "[8, sph (sphalerite), color: #ee82ee]\n",
            "[11, tnt/ttr (tenantite/tetrahedrite), color: #483d8b]\n"
          ]
        }
      ],
      "source": [
        "from petroscope.segmentation.classes import ClassSet, LumenStoneClasses\n",
        "\n",
        "classset = LumenStoneClasses.S1v1()\n",
        "for cl in classset.classes:\n",
        "    print(cl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Несолько подключенний библиотек"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikit-image\n",
            "  Downloading scikit_image-0.25.2-cp313-cp313-macosx_10_13_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy>=1.24 in /Users/antonykorotkov/Documents/geo_proj/geo_env/lib/python3.13/site-packages (from scikit-image) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /Users/antonykorotkov/Documents/geo_proj/geo_env/lib/python3.13/site-packages (from scikit-image) (1.15.2)\n",
            "Collecting networkx>=3.0 (from scikit-image)\n",
            "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: pillow>=10.1 in /Users/antonykorotkov/Documents/geo_proj/geo_env/lib/python3.13/site-packages (from scikit-image) (11.1.0)\n",
            "Collecting imageio!=2.35.0,>=2.33 (from scikit-image)\n",
            "  Downloading imageio-2.37.0-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting tifffile>=2022.8.12 (from scikit-image)\n",
            "  Downloading tifffile-2025.3.30-py3-none-any.whl.metadata (32 kB)\n",
            "Requirement already satisfied: packaging>=21 in /Users/antonykorotkov/Documents/geo_proj/geo_env/lib/python3.13/site-packages (from scikit-image) (24.2)\n",
            "Collecting lazy-loader>=0.4 (from scikit-image)\n",
            "  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
            "Downloading scikit_image-0.25.2-cp313-cp313-macosx_10_13_x86_64.whl (13.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading imageio-2.37.0-py3-none-any.whl (315 kB)\n",
            "Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
            "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tifffile-2025.3.30-py3-none-any.whl (226 kB)\n",
            "Installing collected packages: tifffile, networkx, lazy-loader, imageio, scikit-image\n",
            "Successfully installed imageio-2.37.0 lazy-loader-0.4 networkx-3.4.2 scikit-image-0.25.2 tifffile-2025.3.30\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pickle as pkl\n",
        "\n",
        "from skimage.color import rgb2gray\n",
        "from skimage.filters import sobel\n",
        "from skimage.segmentation import watershed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Класс MyGeoSegmModel\n",
        "Метод load - просто сохранение обьекта через pickle\n",
        "\n",
        "Обучение: запоминание признаков текстуры разных классов - наборы признаков (признаки GLCM и средний цвет) кладутся в KNeighborsClassifier (далее knn), который и будет предсказывать текстуру\n",
        "\n",
        "Предсказание: делим изображения на кластеры через метод Watershed, в кластере берем кусок текстуры и предсказываем через knn класс для всего кластера"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPula--54EpJ"
      },
      "outputs": [],
      "source": [
        "import petroscope.segmentation as segm\n",
        "from dataclasses import dataclass\n",
        "from typing import Iterable\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from petroscope.segmentation.utils import load_image, load_mask\n",
        "from scipy.ndimage import convolve, maximum_filter\n",
        "\n",
        "class MyGeoSegmModel(segm.GeoSegmModel):\n",
        "\n",
        "    #meta parameters for training\n",
        "    block_size = 32\n",
        "    filler_color = [0,0,0] #[-1000, -1000, -1000]\n",
        "    filler_color_grey = 0 #-1000\n",
        "    density_minimum = 0.3\n",
        "    glcm_vector = [4, 0]\n",
        "    glcm_levels = 16\n",
        "    features_count = 4+3+4\n",
        "    knn_parameter = 3\n",
        "\n",
        "    #load\n",
        "    path_to_saved_data = Path('./saved_data/') #'geo_proj_data_16_par.pkl'\n",
        "\n",
        "    #meta parameters for predicting\n",
        "    watershed_markers = 128 #1024\n",
        "    compactness = 0.001\n",
        "    sampler_size = 32\n",
        "    sampler_search_step = 4\n",
        "\n",
        "    def generate_laws_filters(self) -> dict[str, np.ndarray]:\n",
        "        # Определение одномерных масок Лавса\n",
        "        filters_1d = {\n",
        "            \"L5\": np.array([1, 4, 6, 4, 1], dtype=np.float16),   # Level (L)\n",
        "            \"E5\": np.array([-1, -2, 0, 2, 1], dtype=np.float16), # Edge (E)\n",
        "            \"S5\": np.array([-1, 0, 2, 0, -1], dtype=np.float16), # Spot (S)\n",
        "            \"R5\": np.array([1, -4, 6, -4, 1], dtype=np.float16), # Ripple (R)\n",
        "            \"W5\": np.array([-1, 2, 0, -2, 1], dtype=np.float16), # Wave (W)\n",
        "        }\n",
        "        filters_2d = {}\n",
        "        for f_name_1, f_val_1 in filters_1d.items():\n",
        "            for f_name_2, f_val_2 in filters_1d.items():\n",
        "                kernel = np.outer(f_val_1, f_val_2)  # Декартово произведение\n",
        "                name = f\"{f_name_1}{f_name_2}\"\n",
        "                filters_2d[name] = kernel\n",
        "        return filters_2d\n",
        "\n",
        "    def __init__(self, classes: ClassSet) -> None:\n",
        "        super().__init__()\n",
        "        self.classes = classes\n",
        "        self.clusters = dict()\n",
        "        self.scaler = StandardScaler()\n",
        "        self.knn = KNeighborsClassifier(n_neighbors=self.knn_parameter)\n",
        "\n",
        "        laws_filters = self.generate_laws_filters()\n",
        "        self.filter = laws_filters[\"L5E5\"]\n",
        "\n",
        "\n",
        "\n",
        "    def load(self, saved_path = path_to_saved_data, **kwargs) -> None:\n",
        "        self.knn = pkl.load(open(self.path_to_saved_data / f\"knn.pkl\", 'rb'))\n",
        "        self.scaler = pkl.load(open(self.path_to_saved_data / f\"scaler.pkl\", 'rb'))\n",
        "        \n",
        "\n",
        "    def calculate_glcm_features(self, image: np.ndarray, num_levels = glcm_levels, dx = glcm_vector[0], dy = glcm_vector[1]) -> np.ndarray:\n",
        "        #image = np.floor(image / (256 / num_levels)).astype(int)\n",
        "        image = (image * (num_levels-1)).astype(int)\n",
        "        rows, cols = image.shape\n",
        "        glcm = np.zeros((num_levels, num_levels), dtype=int)\n",
        "        for i in range(rows - dy):\n",
        "            for j in range(cols - dx):\n",
        "                x = image[i, j]\n",
        "                y = image[i + dy, j + dx]\n",
        "                # if x >= 0 and y >= 0:\n",
        "                glcm[x, y] += 1\n",
        "        glcmn = glcm.astype(np.float32) / np.sum(glcm)\n",
        "        n = num_levels\n",
        "        glcmn_2 = glcmn[glcmn > 0]  # Убираем нули\n",
        "        entropy = -np.sum(glcmn_2 * np.log(glcmn_2))\n",
        "        contrast = np.sum([(i - j) ** 2 * glcmn[i, j] for i in range(n) for j in range(n)])\n",
        "        homogeneity = np.sum([glcmn[i, j] / (1 + (i - j) ** 2) for i in range(n) for j in range(n)])\n",
        "        dissimilarity = np.sum([abs(i - j) * glcmn[i, j] for i in range(n) for j in range(n)])\n",
        "        return [entropy, contrast, homogeneity, dissimilarity]\n",
        "    \n",
        "\n",
        "\n",
        "    def create_features(self, block: np.ndarray, mask: np.ndarray, code:int, density: np.float32) ->np.ndarray:\n",
        "        grey_block = rgb2gray(block)\n",
        "        grey_block[mask != code] = self.filler_color_grey\n",
        "        color = block[mask == code, :].mean(axis=0)\n",
        "        filtered = np.clip(convolve(grey_block, self.filter, mode='constant', cval=0.0), 0, 1)\n",
        "        \n",
        "        return np.concatenate([self.calculate_glcm_features(grey_block), color, self.calculate_glcm_features(filtered)], axis=-1)\n",
        "\n",
        "    def train(\n",
        "        self, img_mask_paths: Iterable[tuple[Path, Path]], **kwargs\n",
        "    ) -> None:\n",
        "        for img_p, mask_p in tqdm(img_mask_paths):\n",
        "            img = load_image(img_p, normalize=True)\n",
        "            mask = load_mask(mask_p, classes=self.classes, one_hot=False)\n",
        "            #print(\"img\", img)\n",
        "\n",
        "            features_array = np.array([])\n",
        "            code_array = np.array([])\n",
        "\n",
        "            for i in range(0, img.shape[0], self.block_size):\n",
        "                for j in range(0, img.shape[1], self.block_size):\n",
        "                    block = img[i:i+self.block_size, j:j+self.block_size]\n",
        "                    block_mask = mask[i:i+self.block_size, j:j+self.block_size]\n",
        "                    for code in np.unique(block_mask):    \n",
        "                        density = block[block_mask == code].size / block.size\n",
        "                        if density >= self.density_minimum: \n",
        "                            #construct and add new element to knn pool:\n",
        "                            features = self.create_features(block, block_mask, code, density)\n",
        "                            features_array = np.append(features_array, features)\n",
        "                            code_array = np.append(code_array, code)\n",
        "\n",
        "\n",
        "            #print(\"features array: \", features_array)\n",
        "            features_array = features_array.reshape(code_array.shape[0], self.features_count)\n",
        "            features_array = self.scaler.fit_transform(features_array)\n",
        "            self.knn.fit(features_array, code_array)\n",
        "\n",
        "            \n",
        "            #save\n",
        "            self.path_to_saved_data.mkdir(exist_ok=True)\n",
        "            pkl.dump(self.knn, open(self.path_to_saved_data / f\"knn.pkl\", 'wb'), pkl.HIGHEST_PROTOCOL)\n",
        "            pkl.dump(self.scaler, open(self.path_to_saved_data / f\"scaler.pkl\", 'wb'), pkl.HIGHEST_PROTOCOL)\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            \n",
        "\n",
        "        \n",
        "\n",
        "    def predict_image(self, image: np.ndarray) -> np.ndarray:\n",
        "        \n",
        "        # watershed algorithm:\n",
        "        gradient = sobel(rgb2gray(image))\n",
        "        segments_watershed = watershed(gradient, markers=self.watershed_markers, compactness=self.compactness)-1\n",
        "        result = np.zeros(segments_watershed.shape, dtype=int)\n",
        "        cluster_count = len(np.unique(segments_watershed))\n",
        "        # [density and coordinates of corner] of sampler for each cluster\n",
        "        # we want samplers with the biggest density\n",
        "        sampler_density = np.zeros((cluster_count, ), dtype=np.float32)\n",
        "        sampler_coordiantes = np.zeros((cluster_count, 2), dtype=int)\n",
        "\n",
        "        \n",
        "\n",
        "        # chosing samplers\n",
        "        for i in range(0, segments_watershed.shape[0] - self.sampler_size, self.sampler_search_step):\n",
        "                for j in range(0, segments_watershed.shape[1] - self.sampler_size, self.sampler_search_step):\n",
        "                    block = segments_watershed[i:i+self.sampler_size, j:j+self.sampler_size]\n",
        "                    for code in np.unique(block):\n",
        "                        density = block[block == code].size / block.size\n",
        "                        if sampler_density[code] < density:\n",
        "                            sampler_density[code] = density\n",
        "                            sampler_coordiantes[code] = [i, j]\n",
        "        \n",
        "\n",
        "\n",
        "        # predict for each cluster using best sampler (the densest)\n",
        "        for code in range(cluster_count):\n",
        "            density = sampler_density[code]\n",
        "            i, j = sampler_coordiantes[code]\n",
        "            block = image[i:i+self.sampler_size, j:j+self.sampler_size]\n",
        "            mask = segments_watershed[i:i+self.sampler_size, j:j+self.sampler_size]\n",
        "            features = self.create_features(block, mask, code, density)\n",
        "            features = self.scaler.transform([features])\n",
        "            result[segments_watershed == code] = self.knn.predict(features)[0]\n",
        "\n",
        "        return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lr2ltT684EpJ"
      },
      "source": [
        "Тут я использовал балансировщик из petroscope и сохранил патчи в папку `./balanced_data`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m2025-04-08T02:30:07.417441+0300\u001b[0m \u001b[1mINFO\u001b[0m \u001b[36mInitializing dataset...\u001b[0m\n",
            "loading images:  15%|█▌        | 9/59 [00:03<00:20,  2.48it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[310]\u001b[39m\u001b[32m, line 50\u001b[39m\n\u001b[32m     44\u001b[39m             Image.fromarray(msk).save(exp_dir / \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmasks/train/train_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.png\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     46\u001b[39m     \u001b[38;5;28mprint\u001b[39m(ds.accum)\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m \u001b[43mrun_balancer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[310]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mrun_balancer\u001b[39m\u001b[34m(iterations, save_patches)\u001b[39m\n\u001b[32m     18\u001b[39m exp_dir = Path(\u001b[33m\"\u001b[39m\u001b[33m./very_small_balanced_data\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m exp_dir.mkdir(parents=\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m ds = \u001b[43mSelfBalancingDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimg_mask_paths\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimg_mask_pairs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mLumenStone/S1_v1.5/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43maugment_rotation\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43maugment_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcls_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclass_area_consideration\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatch_positioning_accuracy\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbalancing_strength\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.75\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43macceleration\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcache\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m s = ds.sampler_balanced()\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(iterations), \u001b[33m\"\u001b[39m\u001b[33mextracting patches\u001b[39m\u001b[33m\"\u001b[39m):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/geo_proj/geo_env/lib/python3.13/site-packages/petroscope/segmentation/balancer/balancer.py:489\u001b[39m, in \u001b[36mSelfBalancingDataset.__init__\u001b[39m\u001b[34m(self, img_mask_paths, cls_indices, patch_size, void_border_width, balancing_strength, class_area_consideration, patch_positioning_accuracy, acceleration, augment_rotation, augment_scale, cache_dir, print_class_distribution)\u001b[39m\n\u001b[32m    487\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mInitializing dataset...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    488\u001b[39m t1 = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    490\u001b[39m t2 = time.time()\n\u001b[32m    491\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33minitialization took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt2\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mt1\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/geo_proj/geo_env/lib/python3.13/site-packages/petroscope/segmentation/balancer/balancer.py:501\u001b[39m, in \u001b[36mSelfBalancingDataset._initialize\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    497\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_initialize\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    498\u001b[39m \n\u001b[32m    499\u001b[39m     \u001b[38;5;66;03m# create items\u001b[39;00m\n\u001b[32m    500\u001b[39m     \u001b[38;5;28mself\u001b[39m.items = [\n\u001b[32m--> \u001b[39m\u001b[32m501\u001b[39m         \u001b[43mDsItem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvoid_border_width\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    502\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m img_p, mask_p \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mself\u001b[39m.img_mask_paths, \u001b[33m\"\u001b[39m\u001b[33mloading images\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    503\u001b[39m     ]\n\u001b[32m    505\u001b[39m     \u001b[38;5;66;03m# pixels distribution is stored as nested dict:\u001b[39;00m\n\u001b[32m    506\u001b[39m     \u001b[38;5;66;03m# cls_idx -> img_idx -> n_pixels\u001b[39;00m\n\u001b[32m    507\u001b[39m     \u001b[38;5;28mself\u001b[39m.ds_dstr = {cls_idx: \u001b[38;5;28mdict\u001b[39m() \u001b[38;5;28;01mfor\u001b[39;00m cls_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cls_indices}\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/geo_proj/geo_env/lib/python3.13/site-packages/petroscope/segmentation/balancer/balancer.py:69\u001b[39m, in \u001b[36mDsItem.__init__\u001b[39m\u001b[34m(self, img_path, mask_path, void_border_width)\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28mself\u001b[39m.mask_path = mask_path\n\u001b[32m     68\u001b[39m \u001b[38;5;28mself\u001b[39m.void_border_width = void_border_width\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/geo_proj/geo_env/lib/python3.13/site-packages/petroscope/segmentation/balancer/balancer.py:73\u001b[39m, in \u001b[36mDsItem._load_image\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_load_image\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     72\u001b[39m     \u001b[38;5;28mself\u001b[39m.image = np.array(Image.open(\u001b[38;5;28mself\u001b[39m.img_path), dtype=np.uint8)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28mself\u001b[39m.mask = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmask_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mask.ndim == \u001b[32m3\u001b[39m:\n\u001b[32m     75\u001b[39m         \u001b[38;5;28mself\u001b[39m.mask = \u001b[38;5;28mself\u001b[39m.mask[:, :, \u001b[32m0\u001b[39m]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/geo_proj/geo_env/lib/python3.13/site-packages/PIL/Image.py:747\u001b[39m, in \u001b[36mImage.__array_interface__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    745\u001b[39m     new[\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.tobytes(\u001b[33m\"\u001b[39m\u001b[33mraw\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mL\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    746\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m747\u001b[39m     new[\u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    748\u001b[39m new[\u001b[33m\"\u001b[39m\u001b[33mshape\u001b[39m\u001b[33m\"\u001b[39m], new[\u001b[33m\"\u001b[39m\u001b[33mtypestr\u001b[39m\u001b[33m\"\u001b[39m] = _conv_type_shape(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    749\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m new\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/geo_proj/geo_env/lib/python3.13/site-packages/PIL/Image.py:796\u001b[39m, in \u001b[36mImage.tobytes\u001b[39m\u001b[34m(self, encoder_name, *args)\u001b[39m\n\u001b[32m    793\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m encoder_name == \u001b[33m\"\u001b[39m\u001b[33mraw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m encoder_args == ():\n\u001b[32m    794\u001b[39m     encoder_args = \u001b[38;5;28mself\u001b[39m.mode\n\u001b[32m--> \u001b[39m\u001b[32m796\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.width == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.height == \u001b[32m0\u001b[39m:\n\u001b[32m    799\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/geo_proj/geo_env/lib/python3.13/site-packages/PIL/ImageFile.py:300\u001b[39m, in \u001b[36mImageFile.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    297\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[32m    299\u001b[39m b = b + s\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m n, err_code = \u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n < \u001b[32m0\u001b[39m:\n\u001b[32m    302\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "from petroscope.segmentation.balancer.balancer import SelfBalancingDataset\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def img_mask_pairs(ds_dir: Path):\n",
        "    img_dir = ds_dir / \"imgs\" / \"train\"\n",
        "    mask_dir = ds_dir / \"masks\" / \"train\"\n",
        "    img_mask_p = [\n",
        "        (img_p, mask_dir / f\"{img_p.stem}.png\")\n",
        "        for img_p in sorted(img_dir.iterdir())\n",
        "    ]\n",
        "    return img_mask_p\n",
        "\n",
        "\n",
        "def run_balancer(iterations=10, save_patches=True):\n",
        "\n",
        "    exp_dir = Path(\"./very_small_balanced_data\")\n",
        "    exp_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    ds = SelfBalancingDataset(\n",
        "        img_mask_paths=img_mask_pairs(Path(\"LumenStone/S1_v1.5/\")),\n",
        "        patch_size=256,\n",
        "        augment_rotation=30,\n",
        "        augment_scale=0.1,\n",
        "        cls_indices=list(range(16)),\n",
        "        class_area_consideration=1.5,\n",
        "        patch_positioning_accuracy=0.8,\n",
        "        balancing_strength=0.75,\n",
        "        acceleration=8,\n",
        "        cache_dir=Path(\".\") / \"cache\",\n",
        "    )\n",
        "\n",
        "    s = ds.sampler_balanced()\n",
        "    for i in tqdm(range(iterations), \"extracting patches\"):\n",
        "\n",
        "        img, msk = next(s)\n",
        "        if save_patches:\n",
        "            (exp_dir / \"imgs/\").mkdir(exist_ok=True)\n",
        "            (exp_dir / \"imgs/train/\").mkdir(exist_ok=True)\n",
        "            (exp_dir / \"masks/\").mkdir(exist_ok=True)\n",
        "            (exp_dir / \"masks/train/\").mkdir(exist_ok=True)\n",
        "            Image.fromarray(img).save(exp_dir / f\"imgs/train/train_{i+1}.png\")\n",
        "            Image.fromarray(msk).save(exp_dir / f\"masks/train/train_{i+1}.png\")\n",
        "\n",
        "    print(ds.accum)\n",
        "    \n",
        "\n",
        "\n",
        "run_balancer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from petroscope.segmentation.balancer.balancer import SelfBalancingDataset\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "def img_mask_pairs(ds_dir: Path):\n",
        "    img_dir = ds_dir / \"imgs\" / \"train\"\n",
        "    mask_dir = ds_dir / \"masks\" / \"train\"\n",
        "    img_mask_p = [\n",
        "        (img_p, mask_dir / f\"{img_p.stem}.png\")\n",
        "        for img_p in sorted(img_dir.iterdir())\n",
        "    ]\n",
        "    return img_mask_p\n",
        "\n",
        "\n",
        "def run_balancer(iterations=10, save_patches=True):\n",
        "\n",
        "    exp_dir = Path(\"./very_small_balanced_data\")\n",
        "    exp_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    ds = SelfBalancingDataset(\n",
        "        img_mask_paths=img_mask_pairs(Path(\"LumenStone/S1_v1.5/\")),\n",
        "        patch_size=256,\n",
        "        augment_rotation=30,\n",
        "        augment_scale=0.1,\n",
        "        cls_indices=list(range(16)),\n",
        "        class_area_consideration=1.5,\n",
        "        patch_positioning_accuracy=0.8,\n",
        "        balancing_strength=0.75,\n",
        "        acceleration=8,\n",
        "        cache_dir=Path(\".\") / \"cache\",\n",
        "    )\n",
        "\n",
        "    s = ds.sampler_balanced()\n",
        "    for i in tqdm(range(iterations), \"extracting patches\"):\n",
        "\n",
        "        img, msk = next(s)\n",
        "        if save_patches:\n",
        "            (exp_dir / \"imgs/\").mkdir(exist_ok=True)\n",
        "            (exp_dir / \"imgs/train/\").mkdir(exist_ok=True)\n",
        "            (exp_dir / \"masks/\").mkdir(exist_ok=True)\n",
        "            (exp_dir / \"masks/train/\").mkdir(exist_ok=True)\n",
        "            Image.fromarray(img).save(exp_dir / f\"imgs/train/train_{i+1}.png\")\n",
        "            Image.fromarray(msk).save(exp_dir / f\"masks/train/train_{i+1}.png\")\n",
        "\n",
        "    print(ds.accum)\n",
        "    \n",
        "\n",
        "\n",
        "run_balancer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Список пар (путь до изображения, путь до маски) для обучающей (отдельно для сбалансированных данных) и тестовой выборок"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 311,
      "metadata": {
        "id": "tzOzeUlE4EpJ"
      },
      "outputs": [],
      "source": [
        "# fill correct path to the dataset\n",
        "train_img_mask_p = [\n",
        "    (img_p, ds_path / \"masks\" / \"train\" / f\"{img_p.stem}.png\")\n",
        "    for img_p in sorted((ds_path / \"imgs\" / \"train\").iterdir())\n",
        "]\n",
        "\n",
        "balanced_train_img_mask_p = [\n",
        "    (img_p, vsb_path / \"masks\" / \"train\" / f\"{img_p.stem}.png\")\n",
        "    for img_p in sorted((vsb_path / \"imgs\" / \"train\").iterdir())\n",
        "]\n",
        "\n",
        "j_test_img_mask_p = [\n",
        "    (img_p, sb_path / \"masks\" / \"train\" / f\"{img_p.stem}.png\")\n",
        "    for img_p in sorted((sb_path / \"imgs\" / \"train\").iterdir())\n",
        "]\n",
        "\n",
        "test_img_mask_p = [\n",
        "    (img_p, ds_path / \"masks\" / \"test\" / f\"{img_p.stem}.png\")\n",
        "    for img_p in sorted((ds_path / \"imgs\" / \"test\").iterdir())\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Создаем класс "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 342,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUVI8Mde4EpJ",
        "outputId": "d1fb3031-c123-401f-e6a3-1d9cacc3e190"
      },
      "outputs": [],
      "source": [
        "model = MyGeoSegmModel(classes=classset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Обучаем"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 343,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:03<00:00,  2.70it/s]\n"
          ]
        }
      ],
      "source": [
        "#train:\n",
        "model.train(img_mask_paths=balanced_train_img_mask_p[:]) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Или можно загрузить из файла `geo_proj_data.pkl` вместо обучения"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 318,
      "metadata": {},
      "outputs": [],
      "source": [
        "#or load:\n",
        "model.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBAss7ky4EpK"
      },
      "source": [
        "Запуск тестирования модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 345,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LepF12FL4EpK",
        "outputId": "4cbd9c63-b881-4f55-814d-e07fb7e12133"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "testing: 100%|██████████| 1/1 [00:41<00:00, 41.83s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metrics:\n",
            "\t iou [soft]:\n",
            "\t\t bg: 0.0000 [0.0000]\n",
            "\t\t brt: 1.0000 [1.0000]\n",
            "\t\t ccp/kub: 0.0000 [0.0000]\n",
            "\t\t gl: 0.0031 [0.0031]\n",
            "\t\t py/mrc: 0.0000 [0.0000]\n",
            "\t\t sph: 0.0000 [0.0000]\n",
            "\t\t tnt/ttr: 0.0000 [0.0000]\n",
            "\t mean iou [soft]: 0.1433 [0.1433]\n",
            "\t acc: 0.0031\n",
            "\n",
            "Metrics with void borders:\n",
            "\t iou [soft]:\n",
            "\t\t bg: 0.0000 [0.0000]\n",
            "\t\t brt: 1.0000 [1.0000]\n",
            "\t\t ccp/kub: 0.0000 [0.0000]\n",
            "\t\t gl: 0.0029 [0.0029]\n",
            "\t\t py/mrc: 0.0000 [0.0000]\n",
            "\t\t sph: 0.0000 [0.0000]\n",
            "\t\t tnt/ttr: 0.0000 [0.0000]\n",
            "\t mean iou [soft]: 0.1433 [0.1433]\n",
            "\t acc: 0.0828\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from petroscope.segmentation.eval import SegmDetailedTester\n",
        "\n",
        "\n",
        "tester = SegmDetailedTester(\n",
        "    Path(\"output\"),\n",
        "    classes=classset,\n",
        "    void_pad=0,\n",
        "    void_border_width=4,\n",
        "    vis_plots=False,\n",
        "    vis_segmentation=True,\n",
        ")\n",
        "\n",
        "res, res_void = tester.test_on_set(\n",
        "    #j_test_img_mask_p [0:10], #\n",
        "    test_img_mask_p[:1], # remove limit in future!\n",
        "    lambda img: model.predict_image(img),\n",
        "    description=\"test\",\n",
        "    return_void=True,\n",
        ")\n",
        "\n",
        "print(f\"Metrics:\\n{res}\")\n",
        "print(f\"Metrics with void borders:\\n{res_void}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GGppR2H4EpK"
      },
      "source": [
        "В директории ```./output``` по результатам тестирования можете найти визуализацию сегментаций и текстовые файлы со значениями метрик."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxAmDaYF4EpK"
      },
      "source": [
        "## Что нужно теперь сделать?\n",
        "\n",
        "Вам необходимо реализовать собственный метод автоматической сегментиации минералов. Разработанное решение должно быть оформлено в виде класса, отнаследованного от ```GeoSegmModel``` из пакета ```petroscope``` (как в примере выше).\n",
        "\n",
        "Обратите внимание, что реализованный класс должен поддерживать автоматическое сохранение и загрузку модели (метод ```load```). Это позволит протестировать решение, не обучая модель заново.\n",
        "\n",
        "Ваша цель - добиться как можно более высоких показаний метрик сегментации на тестовой выборке (тестовую выборку нельзя использовать при обучении или валидации!), ключевой является метрика mean_iou в режиме void_borders.\n",
        "\n",
        "Решенные задания присылайте в виде ссылок на github репозиторий, или непосредственно ipynb ноутбуки. Обязательно проверьте воспроизводимость кода, чтобы я мог запустить вашу обученную модель!\n",
        "\n",
        "<font color=\"red\">\n",
        "Рекомендуется после получения пайплайна с полными результатами обучения экспортировать ноутбук в pdf (файл -> печать) и положить этот pdf в репозиторий вместе с самим ноутбуком.\n",
        "</font>\n",
        "\n",
        "Удачи!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "geo_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
